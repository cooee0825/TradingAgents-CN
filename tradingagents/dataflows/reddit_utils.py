import time
import json
from datetime import datetime
from typing import Annotated, List, Dict, Optional
import os
import re
from pathlib import Path
import logging
from tqdm import tqdm

# å¯¼å…¥Reddit APIåº“
try:
    import praw

    PRAW_AVAILABLE = True
except ImportError:
    PRAW_AVAILABLE = False
    print("è­¦å‘Š: prawåº“æœªå®‰è£…ï¼ŒRedditä¸‹è½½åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install praw")

ticker_to_company = {
    "AAPL": "Apple",
    "MSFT": "Microsoft",
    "GOOGL": "Google",
    "AMZN": "Amazon",
    "TSLA": "Tesla",
    "NVDA": "Nvidia",
    "TSM": "Taiwan Semiconductor Manufacturing Company OR TSMC",
    "JPM": "JPMorgan Chase OR JP Morgan",
    "JNJ": "Johnson & Johnson OR JNJ",
    "V": "Visa",
    "WMT": "Walmart",
    "META": "Meta OR Facebook",
    "AMD": "AMD",
    "INTC": "Intel",
    "QCOM": "Qualcomm",
    "BABA": "Alibaba",
    "ADBE": "Adobe",
    "NFLX": "Netflix",
    "CRM": "Salesforce",
    "PYPL": "PayPal",
    "PLTR": "Palantir",
    "MU": "Micron",
    "SQ": "Block OR Square",
    "ZM": "Zoom",
    "CSCO": "Cisco",
    "SHOP": "Shopify",
    "ORCL": "Oracle",
    "X": "Twitter OR X",
    "SPOT": "Spotify",
    "AVGO": "Broadcom",
    "ASML": "ASML ",
    "TWLO": "Twilio",
    "SNAP": "Snap Inc.",
    "TEAM": "Atlassian",
    "SQSP": "Squarespace",
    "UBER": "Uber",
    "ROKU": "Roku",
    "PINS": "Pinterest",
}

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# é»˜è®¤subreddité…ç½®
DEFAULT_SUBREDDITS = {
    "global_news": [
        "worldnews",
        "news",
        "business",
        "economy",
        "finance",
        "markets",
        "investing",
    ],
    "company_news": [
        "stocks",
        "investing",
        "SecurityAnalysis",
        "ValueInvesting",
        "StockMarket",
        "wallstreetbets",
        "financialindependence",
    ],
    "crypto_news": [
        "CryptoCurrency",
        "Bitcoin",
        "ethereum",
        "CryptoMarkets",
        "altcoin",
    ],
}


class RedditDataDownloader:
    """Redditæ•°æ®ä¸‹è½½å™¨"""

    def __init__(
        self,
        client_id: Optional[str] = None,
        client_secret: Optional[str] = None,
        user_agent: Optional[str] = None,
        data_dir: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–Redditä¸‹è½½å™¨

        Args:
            client_id: Redditå®¢æˆ·ç«¯ID
            client_secret: Redditå®¢æˆ·ç«¯å¯†é’¥
            user_agent: ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        if not PRAW_AVAILABLE:
            raise ImportError("prawåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install praw")

        # è·å–APIå‡­è¯
        self.client_id = client_id or os.getenv("REDDIT_CLIENT_ID")
        self.client_secret = client_secret or os.getenv("REDDIT_CLIENT_SECRET")
        self.user_agent = user_agent or os.getenv(
            "REDDIT_USER_AGENT", "TradingAgents/1.0"
        )

        if not all([self.client_id, self.client_secret]):
            raise ValueError(
                "Reddit APIå‡­è¯æœªé…ç½®ã€‚è¯·è®¾ç½®REDDIT_CLIENT_IDå’ŒREDDIT_CLIENT_SECRETç¯å¢ƒå˜é‡"
            )

        # è®¾ç½®æ•°æ®ç›®å½•
        self.data_dir = Path(data_dir or "data/reddit_data")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–Reddit APIå®¢æˆ·ç«¯
        try:
            self.reddit = praw.Reddit(
                client_id=self.client_id,
                client_secret=self.client_secret,
                user_agent=self.user_agent,
            )
            # æµ‹è¯•è¿æ¥
            self.reddit.user.me()
            logger.info("âœ… Reddit APIè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Reddit APIè¿æ¥å¤±è´¥: {e}")
            raise

    def download_subreddit_data(
        self,
        subreddit_name: str,
        category: str = "hot",
        limit: int = 100,
        time_filter: str = "week",
    ) -> List[Dict]:
        """
        ä»æŒ‡å®šsubredditä¸‹è½½æ•°æ®

        Args:
            subreddit_name: subredditåç§°
            category: å¸–å­åˆ†ç±» (hot, new, top, rising)
            limit: ä¸‹è½½æ•°é‡é™åˆ¶
            time_filter: æ—¶é—´ç­›é€‰ (all, day, week, month, year) - ä»…å¯¹topæœ‰æ•ˆ

        Returns:
            List[Dict]: å¸–å­æ•°æ®åˆ—è¡¨
        """
        try:
            logger.info(
                f"ğŸ“¥ å¼€å§‹ä¸‹è½½ r/{subreddit_name} çš„{category}å¸–å­ (é™åˆ¶: {limit})"
            )

            subreddit = self.reddit.subreddit(subreddit_name)

            # æ ¹æ®åˆ†ç±»è·å–å¸–å­
            if category == "hot":
                posts = subreddit.hot(limit=limit)
            elif category == "new":
                posts = subreddit.new(limit=limit)
            elif category == "top":
                posts = subreddit.top(time_filter=time_filter, limit=limit)
            elif category == "rising":
                posts = subreddit.rising(limit=limit)
            else:
                logger.warning(f"æœªçŸ¥åˆ†ç±» {category}ï¼Œä½¿ç”¨é»˜è®¤çš„hot")
                posts = subreddit.hot(limit=limit)

            results = []
            for post in posts:
                try:
                    post_data = {
                        "id": post.id,
                        "title": post.title,
                        "selftext": post.selftext,
                        "url": post.url,
                        "ups": post.ups,
                        "downs": getattr(post, "downs", 0),
                        "score": post.score,
                        "upvote_ratio": post.upvote_ratio,
                        "num_comments": post.num_comments,
                        "created_utc": post.created_utc,
                        "author": str(post.author) if post.author else "[deleted]",
                        "subreddit": post.subreddit.display_name,
                        "permalink": f"https://reddit.com{post.permalink}",
                        "is_self": post.is_self,
                        "domain": post.domain,
                        "stickied": post.stickied,
                        "over_18": post.over_18,
                        "spoiler": post.spoiler,
                        "locked": post.locked,
                    }
                    results.append(post_data)

                    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è§¦å‘APIé™åˆ¶
                    time.sleep(0.1)

                except Exception as e:
                    logger.warning(f"è·å–å¸–å­æ•°æ®å¤±è´¥: {e}")
                    continue

            logger.info(f"âœ… æˆåŠŸä¸‹è½½ {len(results)} ä¸ªå¸–å­ä» r/{subreddit_name}")
            return results

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½ r/{subreddit_name} æ•°æ®å¤±è´¥: {e}")
            return []

    def save_posts_to_jsonl(self, posts: List[Dict], file_path: Path) -> bool:
        """
        å°†å¸–å­æ•°æ®ä¿å­˜ä¸ºJSONLæ ¼å¼

        Args:
            posts: å¸–å­æ•°æ®åˆ—è¡¨
            file_path: ä¿å­˜æ–‡ä»¶è·¯å¾„

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            file_path.parent.mkdir(parents=True, exist_ok=True)

            with open(file_path, "w", encoding="utf-8") as f:
                for post in posts:
                    json.dump(post, f, ensure_ascii=False)
                    f.write("\n")

            logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(posts)} ä¸ªå¸–å­åˆ° {file_path}")
            return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False

    def download_category_data(
        self,
        category: str,
        subreddits: Optional[List[str]] = None,
        limit_per_subreddit: int = 100,
        category_type: str = "hot",
        time_filter: str = "week",
        force_refresh: bool = False,
    ) -> bool:
        """
        ä¸‹è½½æŒ‡å®šåˆ†ç±»çš„æ‰€æœ‰subredditæ•°æ®

        Args:
            category: åˆ†ç±»åç§° (global_news, company_news, crypto_news ç­‰)
            subreddits: subredditåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            limit_per_subreddit: æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶
            category_type: å¸–å­åˆ†ç±» (hot, new, top, rising)
            time_filter: æ—¶é—´ç­›é€‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°å·²å­˜åœ¨çš„æ–‡ä»¶

        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä½¿ç”¨æä¾›çš„subredditåˆ—è¡¨æˆ–é»˜è®¤é…ç½®
            if subreddits is None:
                subreddits = DEFAULT_SUBREDDITS.get(category, [])

            if not subreddits:
                logger.error(f"âŒ æœªæ‰¾åˆ°åˆ†ç±» {category} çš„subreddité…ç½®")
                return False

            # åˆ›å»ºåˆ†ç±»ç›®å½•
            category_dir = self.data_dir / category
            category_dir.mkdir(parents=True, exist_ok=True)

            logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½åˆ†ç±» {category} æ•°æ®")
            logger.info(f"ğŸ“‹ Subreddits: {subreddits}")
            logger.info(f"ğŸ“Š æ¯ä¸ªsubreddité™åˆ¶: {limit_per_subreddit}")

            success_count = 0
            total_posts = 0

            with tqdm(subreddits, desc=f"ä¸‹è½½ {category}") as pbar:
                for subreddit_name in pbar:
                    pbar.set_description(f"ä¸‹è½½ r/{subreddit_name}")

                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                    file_path = category_dir / f"{subreddit_name}.jsonl"
                    if file_path.exists() and not force_refresh:
                        logger.info(f"ğŸ“„ æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {file_path}")
                        continue

                    # ä¸‹è½½æ•°æ®
                    posts = self.download_subreddit_data(
                        subreddit_name=subreddit_name,
                        category=category_type,
                        limit=limit_per_subreddit,
                        time_filter=time_filter,
                    )

                    if posts:
                        # ä¿å­˜æ•°æ®
                        if self.save_posts_to_jsonl(posts, file_path):
                            success_count += 1
                            total_posts += len(posts)

                    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
                    time.sleep(1)

            logger.info(f"ğŸ‰ åˆ†ç±» {category} ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“Š æˆåŠŸ: {success_count}/{len(subreddits)} ä¸ªsubreddit")
            logger.info(f"ğŸ“ æ€»è®¡ä¸‹è½½: {total_posts} ä¸ªå¸–å­")

            return success_count > 0

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½åˆ†ç±» {category} å¤±è´¥: {e}")
            return False

    def download_all_categories(
        self,
        limit_per_subreddit: int = 100,
        category_type: str = "hot",
        time_filter: str = "week",
        force_refresh: bool = False,
    ) -> Dict[str, bool]:
        """
        ä¸‹è½½æ‰€æœ‰é¢„é…ç½®åˆ†ç±»çš„æ•°æ®

        Args:
            limit_per_subreddit: æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶
            category_type: å¸–å­åˆ†ç±»
            time_filter: æ—¶é—´ç­›é€‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°

        Returns:
            Dict[str, bool]: å„åˆ†ç±»çš„ä¸‹è½½ç»“æœ
        """
        results = {}

        logger.info("ğŸŒ å¼€å§‹ä¸‹è½½æ‰€æœ‰åˆ†ç±»çš„Redditæ•°æ®")

        for category in DEFAULT_SUBREDDITS.keys():
            logger.info(f"\nğŸ“‚ å¤„ç†åˆ†ç±»: {category}")
            results[category] = self.download_category_data(
                category=category,
                limit_per_subreddit=limit_per_subreddit,
                category_type=category_type,
                time_filter=time_filter,
                force_refresh=force_refresh,
            )

        # è¾“å‡ºæ€»ç»“
        successful = sum(results.values())
        total = len(results)
        logger.info("\nğŸ å…¨éƒ¨ä¸‹è½½å®Œæˆ!")
        logger.info(f"ğŸ“Š æˆåŠŸ: {successful}/{total} ä¸ªåˆ†ç±»")

        return results


def fetch_top_from_category(
    category: Annotated[
        str, "Category to fetch top post from. Collection of subreddits."
    ],
    date: Annotated[str, "Date to fetch top posts from."],
    max_limit: Annotated[int, "Maximum number of posts to fetch."],
    query: Annotated[str, "Optional query to search for in the subreddit."] = None,
    data_path: Annotated[
        str,
        "Path to the data folder. Default is 'reddit_data'.",
    ] = "reddit_data",
):
    base_path = data_path

    all_content = []

    # æ£€æŸ¥åˆ†ç±»ç›®å½•æ˜¯å¦å­˜åœ¨
    category_path = os.path.join(base_path, category)
    if not os.path.exists(category_path):
        logger.warning(f"âš ï¸ Redditæ•°æ®ç›®å½•ä¸å­˜åœ¨: {category_path}")
        return []

    # åªè®¡ç®— .jsonl æ–‡ä»¶çš„æ•°é‡
    jsonl_files = [f for f in os.listdir(category_path) if f.endswith(".jsonl")]

    if len(jsonl_files) == 0:
        logger.warning(f"âš ï¸ åœ¨ {category_path} ä¸­æ²¡æœ‰æ‰¾åˆ° .jsonl æ–‡ä»¶")
        return []

    # ä¿®å¤é€»è¾‘ï¼šç¡®ä¿æ¯ä¸ªsubredditè‡³å°‘å¯ä»¥è·å–1ä¸ªå¸–å­
    if max_limit < len(jsonl_files):
        logger.warning(
            f"âš ï¸ max_limit ({max_limit}) å°äº .jsonl æ–‡ä»¶æ•°é‡ ({len(jsonl_files)})ï¼Œ"
            f"æ¯ä¸ªsubredditåªèƒ½è·å–1ä¸ªå¸–å­"
        )
        limit_per_subreddit = 1
    else:
        limit_per_subreddit = max_limit // len(jsonl_files)

    for data_file in os.listdir(os.path.join(base_path, category)):
        # check if data_file is a .jsonl file
        if not data_file.endswith(".jsonl"):
            continue

        all_content_curr_subreddit = []

        with open(os.path.join(base_path, category, data_file), "rb") as f:
            for i, line in enumerate(f):
                # skip empty lines
                if not line.strip():
                    continue

                parsed_line = json.loads(line)

                # select only lines that are from the date
                post_date = datetime.utcfromtimestamp(
                    parsed_line["created_utc"]
                ).strftime("%Y-%m-%d")
                if post_date != date:
                    continue

                # if is company_news, check that the title or the content has the company's name (query) mentioned
                if "company" in category and query:
                    search_terms = []
                    if "OR" in ticker_to_company[query]:
                        search_terms = ticker_to_company[query].split(" OR ")
                    else:
                        search_terms = [ticker_to_company[query]]

                    search_terms.append(query)

                    found = False
                    for term in search_terms:
                        if re.search(
                            term, parsed_line["title"], re.IGNORECASE
                        ) or re.search(term, parsed_line["selftext"], re.IGNORECASE):
                            found = True
                            break

                    if not found:
                        continue

                post = {
                    "title": parsed_line["title"],
                    "content": parsed_line["selftext"],
                    "url": parsed_line["url"],
                    "upvotes": parsed_line["ups"],
                    "posted_date": post_date,
                }

                all_content_curr_subreddit.append(post)

        # sort all_content_curr_subreddit by upvote_ratio in descending order
        all_content_curr_subreddit.sort(key=lambda x: x["upvotes"], reverse=True)

        all_content.extend(all_content_curr_subreddit[:limit_per_subreddit])

    return all_content


# ä¾¿æ·å‡½æ•°
def download_reddit_data(
    category: str = "all",
    limit_per_subreddit: int = 100,
    category_type: str = "hot",
    time_filter: str = "week",
    force_refresh: bool = False,
    data_dir: Optional[str] = None,
) -> Dict[str, bool]:
    """
    ä¾¿æ·çš„Redditæ•°æ®ä¸‹è½½å‡½æ•°

    Args:
        category: è¦ä¸‹è½½çš„åˆ†ç±» ("all", "global_news", "company_news", "crypto_news")
        limit_per_subreddit: æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶
        category_type: å¸–å­åˆ†ç±» (hot, new, top, rising)
        time_filter: æ—¶é—´ç­›é€‰ (all, day, week, month, year)
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°å·²å­˜åœ¨çš„æ–‡ä»¶
        data_dir: æ•°æ®å­˜å‚¨ç›®å½•

    Returns:
        Dict[str, bool]: ä¸‹è½½ç»“æœ
    """
    try:
        downloader = RedditDataDownloader(data_dir=data_dir)

        if category == "all":
            return downloader.download_all_categories(
                limit_per_subreddit=limit_per_subreddit,
                category_type=category_type,
                time_filter=time_filter,
                force_refresh=force_refresh,
            )
        else:
            result = downloader.download_category_data(
                category=category,
                limit_per_subreddit=limit_per_subreddit,
                category_type=category_type,
                time_filter=time_filter,
                force_refresh=force_refresh,
            )
            return {category: result}

    except Exception as e:
        logger.error(f"âŒ Redditæ•°æ®ä¸‹è½½å¤±è´¥: {e}")
        return {}


def download_custom_subreddits(
    subreddits: List[str],
    category_name: str = "custom",
    limit_per_subreddit: int = 100,
    category_type: str = "hot",
    time_filter: str = "week",
    force_refresh: bool = False,
    data_dir: Optional[str] = None,
) -> bool:
    """
    ä¸‹è½½è‡ªå®šä¹‰subredditåˆ—è¡¨çš„æ•°æ®

    Args:
        subreddits: subredditåç§°åˆ—è¡¨
        category_name: åˆ†ç±»åç§°
        limit_per_subreddit: æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶
        category_type: å¸–å­åˆ†ç±»
        time_filter: æ—¶é—´ç­›é€‰
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
        data_dir: æ•°æ®å­˜å‚¨ç›®å½•

    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    try:
        downloader = RedditDataDownloader(data_dir=data_dir)
        return downloader.download_category_data(
            category=category_name,
            subreddits=subreddits,
            limit_per_subreddit=limit_per_subreddit,
            category_type=category_type,
            time_filter=time_filter,
            force_refresh=force_refresh,
        )
    except Exception as e:
        logger.error(f"âŒ è‡ªå®šä¹‰subredditä¸‹è½½å¤±è´¥: {e}")
        return False


# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
def demo_usage():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Redditä¸‹è½½åŠŸèƒ½"""

    print("ğŸ”§ Redditæ•°æ®ä¸‹è½½å™¨ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)

    # ç¤ºä¾‹1: ä¸‹è½½æ‰€æœ‰é¢„é…ç½®åˆ†ç±»
    print("\nğŸ“¥ ç¤ºä¾‹1: ä¸‹è½½æ‰€æœ‰åˆ†ç±»çš„æ•°æ®")
    print("download_reddit_data(category='all', limit_per_subreddit=50)")

    # ç¤ºä¾‹2: åªä¸‹è½½å…¬å¸æ–°é—»
    print("\nğŸ“¥ ç¤ºä¾‹2: åªä¸‹è½½å…¬å¸æ–°é—»")
    print("download_reddit_data(category='company_news', limit_per_subreddit=100)")

    # ç¤ºä¾‹3: ä¸‹è½½è‡ªå®šä¹‰subreddit
    print("\nğŸ“¥ ç¤ºä¾‹3: ä¸‹è½½è‡ªå®šä¹‰subreddit")
    print(
        "download_custom_subreddits(['wallstreetbets', 'investing'], 'trading_focus')"
    )

    # ç¤ºä¾‹4: ä¸‹è½½æœ€çƒ­é—¨çš„å¸–å­
    print("\nğŸ“¥ ç¤ºä¾‹4: ä¸‹è½½æœ€çƒ­é—¨çš„å¸–å­")
    print(
        "download_reddit_data(category='global_news', category_type='top', time_filter='week')"
    )

    print("\nğŸ’¡ æç¤º:")
    print("1. ç¡®ä¿è®¾ç½®äº†REDDIT_CLIENT_IDå’ŒREDDIT_CLIENT_SECRETç¯å¢ƒå˜é‡")
    print("2. å®‰è£…prawåº“: pip install praw")
    print("3. æ•°æ®å°†ä¿å­˜åˆ°data/reddit_data/ç›®å½•ä¸‹")
    print("4. æ¯ä¸ªsubredditä¿å­˜ä¸ºå•ç‹¬çš„.jsonlæ–‡ä»¶")


if __name__ == "__main__":
    """ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶çš„ç¤ºä¾‹"""
    import argparse

    parser = argparse.ArgumentParser(description="Redditæ•°æ®ä¸‹è½½å·¥å…·")
    parser.add_argument(
        "--category",
        default="company_news",
        choices=["all", "global_news", "company_news", "crypto_news"],
        help="è¦ä¸‹è½½çš„åˆ†ç±»",
    )
    parser.add_argument(
        "--limit", type=int, default=100, help="æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶"
    )
    parser.add_argument(
        "--type",
        default="hot",
        choices=["hot", "new", "top", "rising"],
        help="å¸–å­åˆ†ç±»",
    )
    parser.add_argument(
        "--time-filter",
        default="week",
        choices=["all", "day", "week", "month", "year"],
        help="æ—¶é—´ç­›é€‰",
    )
    parser.add_argument(
        "--force-refresh", action="store_true", help="å¼ºåˆ¶åˆ·æ–°å·²å­˜åœ¨çš„æ–‡ä»¶"
    )
    parser.add_argument("--demo", action="store_true", help="æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹")
    parser.add_argument("--data-dir", default=None, help="æ•°æ®å­˜å‚¨ç›®å½•")

    args = parser.parse_args()

    if args.demo:
        demo_usage()
    else:
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )

        print("ğŸš€ å¼€å§‹ä¸‹è½½Redditæ•°æ®")
        print(f"ğŸ“‚ åˆ†ç±»: {args.category}")
        print(f"ğŸ“Š é™åˆ¶: {args.limit}")
        print(f"ğŸ·ï¸ ç±»å‹: {args.type}")
        print(f"â° æ—¶é—´ç­›é€‰: {args.time_filter}")
        print(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°: {args.force_refresh}")

        results = download_reddit_data(
            category=args.category,
            limit_per_subreddit=args.limit,
            category_type=args.type,
            time_filter=args.time_filter,
            force_refresh=args.force_refresh,
            data_dir=args.data_dir,
        )

        print("\nğŸ‰ ä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“Š ç»“æœ: {results}")
