import time
import json
from datetime import datetime
from typing import Annotated, List, Dict, Optional
import os
import re
from pathlib import Path
import logging
from sqlalchemy import false
from tqdm import tqdm

# å¯¼å…¥Reddit APIåº“
try:
    import praw

    PRAW_AVAILABLE = True
except ImportError:
    PRAW_AVAILABLE = False
    print("è­¦å‘Š: prawåº“æœªå®‰è£…ï¼ŒRedditä¸‹è½½åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install praw")

ticker_to_company = {
    "AAPL": "Apple",
    "MSFT": "Microsoft",
    "GOOGL": "Google",
    "AMZN": "Amazon",
    "TSLA": "Tesla",
    "NVDA": "Nvidia",
    "TSM": "Taiwan Semiconductor Manufacturing Company OR TSMC",
    "JPM": "JPMorgan Chase OR JP Morgan",
    "JNJ": "Johnson & Johnson OR JNJ",
    "V": "Visa",
    "WMT": "Walmart",
    "META": "Meta OR Facebook",
    "AMD": "AMD",
    "INTC": "Intel",
    "QCOM": "Qualcomm",
    "BABA": "Alibaba",
    "ADBE": "Adobe",
    "NFLX": "Netflix",
    "CRM": "Salesforce",
    "PYPL": "PayPal",
    "PLTR": "Palantir",
    "MU": "Micron",
    "SQ": "Block OR Square",
    "ZM": "Zoom",
    "CSCO": "Cisco",
    "SHOP": "Shopify",
    "ORCL": "Oracle",
    "X": "Twitter OR X",
    "SPOT": "Spotify",
    "AVGO": "Broadcom",
    "ASML": "ASML ",
    "TWLO": "Twilio",
    "SNAP": "Snap Inc.",
    "TEAM": "Atlassian",
    "SQSP": "Squarespace",
    "UBER": "Uber",
    "ROKU": "Roku",
    "PINS": "Pinterest",
}

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# é»˜è®¤subreddité…ç½®
DEFAULT_SUBREDDITS = {
    "global_news": [
        "worldnews",
        "news",
        "business",
        "economy",
        "finance",
        "markets",
        "investing",
    ],
    "company_news": [
        "stocks",
        "investing",
        "SecurityAnalysis",
        "ValueInvesting",
        "StockMarket",
        "wallstreetbets",
        "financialindependence",
    ],
    "crypto_news": [
        "CryptoCurrency",
        "Bitcoin",
        "ethereum",
        "CryptoMarkets",
        "altcoin",
    ],
}

# è‚¡ç¥¨ä¸“ç”¨subreddité…ç½®
STOCK_SUBREDDITS = [
    "wallstreetbets",
    "stocks",
    "investing",
    "StockMarket",
    "SecurityAnalysis",
    "ValueInvesting",
]

# subredditæƒé‡é…ç½® (ç”¨äºçƒ­åº¦è®¡ç®—)
SUBREDDIT_WEIGHTS = {
    "wallstreetbets": 1.0,  # å½±å“åŠ›æœ€å¤§
    "stocks": 0.8,
    "investing": 0.7,
    "StockMarket": 0.6,
    "SecurityAnalysis": 0.5,
    "ValueInvesting": 0.4,
}


class RedditDataDownloader:
    """Redditæ•°æ®ä¸‹è½½å™¨"""

    def __init__(
        self,
        client_id: Optional[str] = None,
        client_secret: Optional[str] = None,
        user_agent: Optional[str] = None,
        data_dir: Optional[str] = None,
    ):
        """
        åˆå§‹åŒ–Redditä¸‹è½½å™¨

        Args:
            client_id: Redditå®¢æˆ·ç«¯ID
            client_secret: Redditå®¢æˆ·ç«¯å¯†é’¥
            user_agent: ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        if not PRAW_AVAILABLE:
            raise ImportError("prawåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install praw")

        # è·å–APIå‡­è¯
        self.client_id = client_id or os.getenv("REDDIT_CLIENT_ID")
        self.client_secret = client_secret or os.getenv("REDDIT_CLIENT_SECRET")
        self.user_agent = user_agent or os.getenv(
            "REDDIT_USER_AGENT", "TradingAgents/1.0"
        )

        if not all([self.client_id, self.client_secret]):
            raise ValueError(
                "Reddit APIå‡­è¯æœªé…ç½®ã€‚è¯·è®¾ç½®REDDIT_CLIENT_IDå’ŒREDDIT_CLIENT_SECRETç¯å¢ƒå˜é‡"
            )

        # è®¾ç½®æ•°æ®ç›®å½•
        self.data_dir = Path(data_dir or "data/reddit_data")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–Reddit APIå®¢æˆ·ç«¯
        try:
            self.reddit = praw.Reddit(
                client_id=self.client_id,
                client_secret=self.client_secret,
                user_agent=self.user_agent,
            )
            # æµ‹è¯•è¿æ¥
            self.reddit.user.me()
            logger.info("âœ… Reddit APIè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Reddit APIè¿æ¥å¤±è´¥: {e}")
            raise

    def download_subreddit_data(
        self,
        subreddit_name: str,
        category: str = "hot",
        limit: int = 100,
        time_filter: str = "week",
    ) -> List[Dict]:
        """
        ä»æŒ‡å®šsubredditä¸‹è½½æ•°æ®

        Args:
            subreddit_name: subredditåç§°
            category: å¸–å­åˆ†ç±» (hot, new, top, rising)
            limit: ä¸‹è½½æ•°é‡é™åˆ¶
            time_filter: æ—¶é—´ç­›é€‰ (all, day, week, month, year) - ä»…å¯¹topæœ‰æ•ˆ

        Returns:
            List[Dict]: å¸–å­æ•°æ®åˆ—è¡¨
        """
        try:
            logger.info(
                f"ğŸ“¥ å¼€å§‹ä¸‹è½½ r/{subreddit_name} çš„{category}å¸–å­ (é™åˆ¶: {limit})"
            )

            subreddit = self.reddit.subreddit(subreddit_name)

            # æ ¹æ®åˆ†ç±»è·å–å¸–å­
            if category == "hot":
                posts = subreddit.hot(limit=limit)
            elif category == "new":
                posts = subreddit.new(limit=limit)
            elif category == "top":
                posts = subreddit.top(time_filter=time_filter, limit=limit)
            elif category == "rising":
                posts = subreddit.rising(limit=limit)
            else:
                logger.warning(f"æœªçŸ¥åˆ†ç±» {category}ï¼Œä½¿ç”¨é»˜è®¤çš„hot")
                posts = subreddit.hot(limit=limit)

            results = []
            for post in posts:
                try:
                    post_data = {
                        "id": post.id,
                        "title": post.title,
                        "selftext": post.selftext,
                        "url": post.url,
                        "ups": post.ups,
                        "downs": getattr(post, "downs", 0),
                        "score": post.score,
                        "upvote_ratio": post.upvote_ratio,
                        "num_comments": post.num_comments,
                        "created_utc": post.created_utc,
                        "author": str(post.author) if post.author else "[deleted]",
                        "subreddit": post.subreddit.display_name,
                        "permalink": f"https://reddit.com{post.permalink}",
                        "is_self": post.is_self,
                        "domain": post.domain,
                        "stickied": post.stickied,
                        "over_18": post.over_18,
                        "spoiler": post.spoiler,
                        "locked": post.locked,
                    }
                    results.append(post_data)

                    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è§¦å‘APIé™åˆ¶
                    time.sleep(0.1)

                except Exception as e:
                    logger.warning(f"è·å–å¸–å­æ•°æ®å¤±è´¥: {e}")
                    continue

            logger.info(f"âœ… æˆåŠŸä¸‹è½½ {len(results)} ä¸ªå¸–å­ä» r/{subreddit_name}")
            return results

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½ r/{subreddit_name} æ•°æ®å¤±è´¥: {e}")
            return []

    def save_posts_to_jsonl(self, posts: List[Dict], file_path: Path) -> bool:
        """
        å°†å¸–å­æ•°æ®ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼Œæ”¯æŒå»é‡å’Œå¢é‡æ›´æ–°

        Args:
            posts: å¸–å­æ•°æ®åˆ—è¡¨
            file_path: ä¿å­˜æ–‡ä»¶è·¯å¾„

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            file_path.parent.mkdir(parents=True, exist_ok=True)

            # è¯»å–ç°æœ‰æ•°æ®
            existing_posts = {}
            if file_path.exists():
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                try:
                                    post_data = json.loads(line)
                                    if "id" in post_data:
                                        existing_posts[post_data["id"]] = post_data
                                except json.JSONDecodeError as e:
                                    logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆçš„JSONè¡Œ: {e}")
                                    continue
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–ç°æœ‰æ–‡ä»¶å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {e}")

            # å¤„ç†æ–°å¸–å­æ•°æ®
            new_posts = 0
            updated_posts = 0
            skipped_posts = 0

            for post in posts:
                if "id" not in post:
                    logger.warning("âš ï¸ å¸–å­ç¼ºå°‘IDï¼Œè·³è¿‡")
                    continue

                post_id = post["id"]

                if post_id in existing_posts:
                    # æ¯”è¾ƒå¸–å­å†…å®¹æ˜¯å¦æœ‰æ›´æ–°
                    existing_post = existing_posts[post_id]

                    # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦æœ‰å˜åŒ–
                    key_fields = [
                        "title",
                        "selftext",
                        "score",
                        "ups",
                        "num_comments",
                        "upvote_ratio",
                    ]
                    has_changes = False

                    for field in key_fields:
                        if post.get(field) != existing_post.get(field):
                            has_changes = True
                            break

                    if has_changes:
                        # æ›´æ–°å¸–å­æ•°æ®ï¼Œä¿ç•™åˆ›å»ºæ—¶é—´ç­‰åŸæœ‰ä¿¡æ¯
                        updated_post = existing_post.copy()
                        updated_post.update(post)
                        updated_post["last_updated"] = datetime.now().isoformat()
                        existing_posts[post_id] = updated_post
                        updated_posts += 1
                        logger.debug(f"ğŸ”„ æ›´æ–°å¸–å­: {post_id}")
                    else:
                        # å†…å®¹æ— å˜åŒ–ï¼Œè·³è¿‡
                        skipped_posts += 1
                        logger.debug(f"â­ï¸ å¸–å­æ— å˜åŒ–ï¼Œè·³è¿‡: {post_id}")
                else:
                    # æ–°å¸–å­
                    post["first_saved"] = datetime.now().isoformat()
                    existing_posts[post_id] = post
                    new_posts += 1
                    logger.debug(f"â• æ–°å¢å¸–å­: {post_id}")

            # ä¿å­˜æ‰€æœ‰æ•°æ®
            with open(file_path, "w", encoding="utf-8") as f:
                for post_data in existing_posts.values():
                    json.dump(post_data, f, ensure_ascii=False)
                    f.write("\n")

            total_posts = len(existing_posts)
            logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜åˆ° {file_path}")
            logger.info(
                f"ğŸ“Š ç»Ÿè®¡: æ€»è®¡ {total_posts} ä¸ªå¸–å­ (æ–°å¢ {new_posts}, æ›´æ–° {updated_posts}, è·³è¿‡ {skipped_posts})"
            )

            return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False

    def download_category_data(
        self,
        category: str,
        subreddits: Optional[List[str]] = None,
        limit_per_subreddit: int = 100,
        category_type: str = "hot",
        time_filter: str = "week",
        force_refresh: bool = False,
    ) -> bool:
        """
        ä¸‹è½½æŒ‡å®šåˆ†ç±»çš„æ‰€æœ‰subredditæ•°æ®

        Args:
            category: åˆ†ç±»åç§° (global_news, company_news, crypto_news ç­‰)
            subreddits: subredditåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            limit_per_subreddit: æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶
            category_type: å¸–å­åˆ†ç±» (hot, new, top, rising)
            time_filter: æ—¶é—´ç­›é€‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼Œç”±äºå·²æ”¯æŒå¢é‡æ›´æ–°ï¼Œæ­¤å‚æ•°ä¸»è¦ç”¨äºæ—¥å¿—æ˜¾ç¤º

        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä½¿ç”¨æä¾›çš„subredditåˆ—è¡¨æˆ–é»˜è®¤é…ç½®
            if subreddits is None:
                subreddits = DEFAULT_SUBREDDITS.get(category, [])

            if not subreddits:
                logger.error(f"âŒ æœªæ‰¾åˆ°åˆ†ç±» {category} çš„subreddité…ç½®")
                return False

            # åˆ›å»ºåˆ†ç±»ç›®å½•
            category_dir = self.data_dir / category
            category_dir.mkdir(parents=True, exist_ok=True)

            logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½åˆ†ç±» {category} æ•°æ®")
            logger.info(f"ğŸ“‹ Subreddits: {subreddits}")
            logger.info(f"ğŸ“Š æ¯ä¸ªsubreddité™åˆ¶: {limit_per_subreddit}")

            success_count = 0
            total_posts = 0

            with tqdm(subreddits, desc=f"ä¸‹è½½ {category}") as pbar:
                for subreddit_name in pbar:
                    pbar.set_description(f"ä¸‹è½½ r/{subreddit_name}")

                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                    file_path = category_dir / f"{subreddit_name}.jsonl"

                    # ä¸‹è½½æ•°æ®
                    posts = self.download_subreddit_data(
                        subreddit_name=subreddit_name,
                        category=category_type,
                        limit=limit_per_subreddit,
                        time_filter=time_filter,
                    )

                    if posts:
                        # ä¿å­˜æ•°æ®
                        if self.save_posts_to_jsonl(posts, file_path):
                            success_count += 1
                            total_posts += len(posts)

                    # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
                    time.sleep(1)

            logger.info(f"ğŸ‰ åˆ†ç±» {category} ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“Š æˆåŠŸ: {success_count}/{len(subreddits)} ä¸ªsubreddit")
            logger.info(f"ğŸ“ æ€»è®¡ä¸‹è½½: {total_posts} ä¸ªå¸–å­")

            return success_count > 0

        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½åˆ†ç±» {category} å¤±è´¥: {e}")
            return False

    def download_all_categories(
        self,
        limit_per_subreddit: int = 100,
        category_type: str = "hot",
        time_filter: str = "week",
        force_refresh: bool = False,
    ) -> Dict[str, bool]:
        """
        ä¸‹è½½æ‰€æœ‰é¢„é…ç½®åˆ†ç±»çš„æ•°æ®

        Args:
            limit_per_subreddit: æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶
            category_type: å¸–å­åˆ†ç±»
            time_filter: æ—¶é—´ç­›é€‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°

        Returns:
            Dict[str, bool]: å„åˆ†ç±»çš„ä¸‹è½½ç»“æœ
        """
        results = {}

        logger.info("ğŸŒ å¼€å§‹ä¸‹è½½æ‰€æœ‰åˆ†ç±»çš„Redditæ•°æ®")

        for category in DEFAULT_SUBREDDITS.keys():
            logger.info(f"\nğŸ“‚ å¤„ç†åˆ†ç±»: {category}")
            results[category] = self.download_category_data(
                category=category,
                limit_per_subreddit=limit_per_subreddit,
                category_type=category_type,
                time_filter=time_filter,
                force_refresh=force_refresh,
            )

        # è¾“å‡ºæ€»ç»“
        successful = sum(results.values())
        total = len(results)
        logger.info("\nğŸ å…¨éƒ¨ä¸‹è½½å®Œæˆ!")
        logger.info(f"ğŸ“Š æˆåŠŸ: {successful}/{total} ä¸ªåˆ†ç±»")

        return results


def fetch_top_from_category(
    category: Annotated[
        str, "Category to fetch top post from. Collection of subreddits."
    ],
    date: Annotated[str, "Date to fetch top posts from."],
    max_limit: Annotated[int, "Maximum number of posts to fetch."],
    query: Annotated[str, "Optional query to search for in the subreddit."] = None,
    data_path: Annotated[
        str,
        "Path to the data folder. Default is 'reddit_data'.",
    ] = "reddit_data",
):
    base_path = data_path

    all_content = []

    # æ£€æŸ¥åˆ†ç±»ç›®å½•æ˜¯å¦å­˜åœ¨
    category_path = os.path.join(base_path, category)
    if not os.path.exists(category_path):
        logger.warning(f"âš ï¸ Redditæ•°æ®ç›®å½•ä¸å­˜åœ¨: {category_path}")
        return []

    # åªè®¡ç®— .jsonl æ–‡ä»¶çš„æ•°é‡
    jsonl_files = [f for f in os.listdir(category_path) if f.endswith(".jsonl")]

    if len(jsonl_files) == 0:
        logger.warning(f"âš ï¸ åœ¨ {category_path} ä¸­æ²¡æœ‰æ‰¾åˆ° .jsonl æ–‡ä»¶")
        return []

    # ä¿®å¤é€»è¾‘ï¼šç¡®ä¿æ¯ä¸ªsubredditè‡³å°‘å¯ä»¥è·å–1ä¸ªå¸–å­
    if max_limit < len(jsonl_files):
        logger.warning(
            f"âš ï¸ max_limit ({max_limit}) å°äº .jsonl æ–‡ä»¶æ•°é‡ ({len(jsonl_files)})ï¼Œ"
            f"æ¯ä¸ªsubredditåªèƒ½è·å–1ä¸ªå¸–å­"
        )
        limit_per_subreddit = 1
    else:
        limit_per_subreddit = max_limit // len(jsonl_files)

    for data_file in os.listdir(os.path.join(base_path, category)):
        # check if data_file is a .jsonl file
        if not data_file.endswith(".jsonl"):
            continue

        all_content_curr_subreddit = []

        with open(os.path.join(base_path, category, data_file), "rb") as f:
            for i, line in enumerate(f):
                # skip empty lines
                if not line.strip():
                    continue

                parsed_line = json.loads(line)

                # select only lines that are from the date
                post_date = datetime.utcfromtimestamp(
                    parsed_line["created_utc"]
                ).strftime("%Y-%m-%d")
                if post_date != date:
                    continue

                # if is company_news, check that the title or the content has the company's name (query) mentioned
                if "company" in category and query:
                    search_terms = []
                    if "OR" in ticker_to_company[query]:
                        search_terms = ticker_to_company[query].split(" OR ")
                    else:
                        search_terms = [ticker_to_company[query]]

                    search_terms.append(query)

                    found = False
                    for term in search_terms:
                        if re.search(
                            term, parsed_line["title"], re.IGNORECASE
                        ) or re.search(term, parsed_line["selftext"], re.IGNORECASE):
                            found = True
                            break

                    if not found:
                        continue

                post = {
                    "title": parsed_line["title"],
                    "content": parsed_line["selftext"],
                    "url": parsed_line["url"],
                    "upvotes": parsed_line["ups"],
                    "posted_date": post_date,
                }

                all_content_curr_subreddit.append(post)

        # sort all_content_curr_subreddit by upvote_ratio in descending order
        all_content_curr_subreddit.sort(key=lambda x: x["upvotes"], reverse=True)

        all_content.extend(all_content_curr_subreddit[:limit_per_subreddit])

    return all_content


# ä¾¿æ·å‡½æ•°
def download_reddit_data(
    category: str = "all",
    limit_per_subreddit: int = 100,
    category_type: str = "hot",
    time_filter: str = "week",
    force_refresh: bool = False,
    data_dir: Optional[str] = None,
) -> Dict[str, bool]:
    """
    ä¾¿æ·çš„Redditæ•°æ®ä¸‹è½½å‡½æ•°

    Args:
        category: è¦ä¸‹è½½çš„åˆ†ç±» ("all", "global_news", "company_news", "crypto_news")
        limit_per_subreddit: æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶
        category_type: å¸–å­åˆ†ç±» (hot, new, top, rising)
        time_filter: æ—¶é—´ç­›é€‰ (all, day, week, month, year)
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°å·²å­˜åœ¨çš„æ–‡ä»¶
        data_dir: æ•°æ®å­˜å‚¨ç›®å½•

    Returns:
        Dict[str, bool]: ä¸‹è½½ç»“æœ
    """
    try:
        downloader = RedditDataDownloader(data_dir=data_dir)

        if category == "all":
            return downloader.download_all_categories(
                limit_per_subreddit=limit_per_subreddit,
                category_type=category_type,
                time_filter=time_filter,
                force_refresh=force_refresh,
            )
        else:
            result = downloader.download_category_data(
                category=category,
                limit_per_subreddit=limit_per_subreddit,
                category_type=category_type,
                time_filter=time_filter,
                force_refresh=force_refresh,
            )
            return {category: result}

    except Exception as e:
        logger.error(f"âŒ Redditæ•°æ®ä¸‹è½½å¤±è´¥: {e}")
        return {}


def download_custom_subreddits(
    subreddits: List[str],
    category_name: str = "custom",
    limit_per_subreddit: int = 100,
    category_type: str = "hot",
    time_filter: str = "week",
    force_refresh: bool = False,
    data_dir: Optional[str] = None,
) -> bool:
    """
    ä¸‹è½½è‡ªå®šä¹‰subredditåˆ—è¡¨çš„æ•°æ®

    Args:
        subreddits: subredditåç§°åˆ—è¡¨
        category_name: åˆ†ç±»åç§°
        limit_per_subreddit: æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶
        category_type: å¸–å­åˆ†ç±»
        time_filter: æ—¶é—´ç­›é€‰
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°
        data_dir: æ•°æ®å­˜å‚¨ç›®å½•

    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    try:
        downloader = RedditDataDownloader(data_dir=data_dir)
        return downloader.download_category_data(
            category=category_name,
            subreddits=subreddits,
            limit_per_subreddit=limit_per_subreddit,
            category_type=category_type,
            time_filter=time_filter,
            force_refresh=force_refresh,
        )
    except Exception as e:
        logger.error(f"âŒ è‡ªå®šä¹‰subredditä¸‹è½½å¤±è´¥: {e}")
        return False


# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
def demo_usage():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨Redditä¸‹è½½åŠŸèƒ½"""

    print("ğŸ”§ Redditæ•°æ®ä¸‹è½½å™¨ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)

    # ç¤ºä¾‹1: ä¸‹è½½æ‰€æœ‰é¢„é…ç½®åˆ†ç±»
    print("\nğŸ“¥ ç¤ºä¾‹1: ä¸‹è½½æ‰€æœ‰åˆ†ç±»çš„æ•°æ®")
    print("download_reddit_data(category='all', limit_per_subreddit=50)")

    # ç¤ºä¾‹2: åªä¸‹è½½å…¬å¸æ–°é—»
    print("\nğŸ“¥ ç¤ºä¾‹2: åªä¸‹è½½å…¬å¸æ–°é—»")
    print("download_reddit_data(category='company_news', limit_per_subreddit=100)")

    # ç¤ºä¾‹3: ä¸‹è½½è‡ªå®šä¹‰subreddit
    print("\nğŸ“¥ ç¤ºä¾‹3: ä¸‹è½½è‡ªå®šä¹‰subreddit")
    print(
        "download_custom_subreddits(['wallstreetbets', 'investing'], 'trading_focus')"
    )

    # ç¤ºä¾‹4: ä¸‹è½½æœ€çƒ­é—¨çš„å¸–å­
    print("\nğŸ“¥ ç¤ºä¾‹4: ä¸‹è½½æœ€çƒ­é—¨çš„å¸–å­")
    print(
        "download_reddit_data(category='global_news', category_type='top', time_filter='week')"
    )

    print("\nğŸ’¡ æç¤º:")
    print("1. ç¡®ä¿è®¾ç½®äº†REDDIT_CLIENT_IDå’ŒREDDIT_CLIENT_SECRETç¯å¢ƒå˜é‡")
    print("2. å®‰è£…prawåº“: pip install praw")
    print("3. æ•°æ®å°†ä¿å­˜åˆ°data/reddit_data/ç›®å½•ä¸‹")
    print("4. æ¯ä¸ªsubredditä¿å­˜ä¸ºå•ç‹¬çš„.jsonlæ–‡ä»¶")


if __name__ == "__main__":
    """ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶çš„ç¤ºä¾‹"""
    import argparse

    parser = argparse.ArgumentParser(description="Redditæ•°æ®ä¸‹è½½å·¥å…·")
    parser.add_argument(
        "--category",
        default="company_news",
        choices=["all", "global_news", "company_news", "crypto_news"],
        help="è¦ä¸‹è½½çš„åˆ†ç±»",
    )
    parser.add_argument(
        "--limit", type=int, default=100, help="æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶"
    )
    parser.add_argument(
        "--type",
        default="hot",
        choices=["hot", "new", "top", "rising"],
        help="å¸–å­åˆ†ç±»",
    )
    parser.add_argument(
        "--time-filter",
        default="week",
        choices=["all", "day", "week", "month", "year"],
        help="æ—¶é—´ç­›é€‰",
    )
    parser.add_argument(
        "--force-refresh", action="store_true", help="å¼ºåˆ¶åˆ·æ–°å·²å­˜åœ¨çš„æ–‡ä»¶"
    )
    parser.add_argument("--demo", action="store_true", help="æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹")
    parser.add_argument("--data-dir", default=None, help="æ•°æ®å­˜å‚¨ç›®å½•")

    args = parser.parse_args()

    if args.demo:
        demo_usage()
    else:
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )

        print("ğŸš€ å¼€å§‹ä¸‹è½½Redditæ•°æ®")
        print(f"ğŸ“‚ åˆ†ç±»: {args.category}")
        print(f"ğŸ“Š é™åˆ¶: {args.limit}")
        print(f"ğŸ·ï¸ ç±»å‹: {args.type}")
        print(f"â° æ—¶é—´ç­›é€‰: {args.time_filter}")
        print(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°: {args.force_refresh}")

        results = download_reddit_data(
            category=args.category,
            limit_per_subreddit=args.limit,
            category_type=args.type,
            time_filter=args.time_filter,
            force_refresh=args.force_refresh,
            data_dir=args.data_dir,
        )

        print("\nğŸ‰ ä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“Š ç»“æœ: {results}")


class StockPopularityAnalyzer:
    """è‚¡ç¥¨çƒ­åº¦åˆ†æå™¨"""

    def __init__(self, data_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–è‚¡ç¥¨çƒ­åº¦åˆ†æå™¨

        Args:
            data_dir: Redditæ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = Path(data_dir or "data/reddit_data")

    def generate_stock_keywords(self, ticker: str) -> List[str]:
        """
        ç”Ÿæˆè‚¡ç¥¨çš„å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºåŒ¹é…

        Args:
            ticker: è‚¡ç¥¨ä»£ç  (å¦‚ "AAPL")

        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        keywords = []

        # åŸºç¡€è‚¡ç¥¨ä»£ç åŒ¹é…
        keywords.extend(
            [
                ticker,
                f"${ticker}",
                f"${ticker.upper()}",
                f"{ticker.upper()}",
                f"{ticker.lower()}",
            ]
        )

        # ä»æ˜ å°„è¡¨è·å–å…¬å¸åç§°
        if ticker in ticker_to_company:
            company_names = ticker_to_company[ticker]
            if " OR " in company_names:
                # å¤„ç†å¤šä¸ªåç§°çš„æƒ…å†µ
                for name in company_names.split(" OR "):
                    keywords.append(name.strip())
            else:
                keywords.append(company_names)

        # å»é‡å¹¶è¿”å›
        return list(set(keywords))

    def calculate_post_relevance(self, post: Dict, keywords: List[str]) -> float:
        """
        è®¡ç®—å¸–å­ä¸è‚¡ç¥¨çš„ç›¸å…³åº¦

        Args:
            post: å¸–å­æ•°æ®
            keywords: å…³é”®è¯åˆ—è¡¨

        Returns:
            float: ç›¸å…³åº¦åˆ†æ•° (0-1)
        """
        title = post.get("title", "").lower()
        content = post.get("selftext", "").lower()

        # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
        title_matches = 0
        content_matches = 0

        for keyword in keywords:
            keyword_lower = keyword.lower()

            # ç²¾ç¡®åŒ¹é…
            if keyword_lower in title:
                title_matches += 1
            if keyword_lower in content:
                content_matches += 1

            # å•è¯è¾¹ç•ŒåŒ¹é… (é¿å…éƒ¨åˆ†åŒ¹é…)
            import re

            pattern = r"\b" + re.escape(keyword_lower) + r"\b"
            if re.search(pattern, title):
                title_matches += 2  # æ›´é«˜æƒé‡
            if re.search(pattern, content):
                content_matches += 1

        # è®¡ç®—ç›¸å…³åº¦åˆ†æ•°
        title_score = min(title_matches * 0.3, 1.0)  # æ ‡é¢˜æœ€é«˜è´¡çŒ®0.3
        content_score = min(content_matches * 0.1, 0.7)  # å†…å®¹æœ€é«˜è´¡çŒ®0.7

        return min(title_score + content_score, 1.0)

    def calculate_post_popularity_score(
        self, post: Dict, subreddit_weight: float = 1.0
    ) -> float:
        """
        è®¡ç®—å¸–å­çš„çƒ­åº¦åˆ†æ•°

        Args:
            post: å¸–å­æ•°æ®
            subreddit_weight: subredditæƒé‡

        Returns:
            float: çƒ­åº¦åˆ†æ•°
        """
        # åŸºç¡€äº’åŠ¨æ•°æ®
        ups = post.get("ups", 0)
        comments = post.get("num_comments", 0)
        score = post.get("score", 0)
        upvote_ratio = post.get("upvote_ratio", 0.5)

        # æ—¶é—´è¡°å‡å› å­ (è¶Šæ–°çš„å¸–å­æƒé‡è¶Šé«˜)
        created_utc = post.get("created_utc", 0)
        current_time = time.time()
        time_diff_hours = (current_time - created_utc) / 3600

        # 24å°æ—¶å†…ä¸º1.0ï¼Œä¹‹åé€æ¸è¡°å‡
        time_decay = max(0.1, 1.0 / (1 + time_diff_hours / 24))

        # è®¡ç®—åŸºç¡€çƒ­åº¦åˆ†æ•°
        engagement_score = (ups * 1.0) + (comments * 0.8) + (score * 0.6)
        quality_score = upvote_ratio * 0.5

        # ç»¼åˆåˆ†æ•°
        total_score = (engagement_score + quality_score) * subreddit_weight * time_decay

        return total_score

    def analyze_stock_popularity(
        self,
        ticker: str,
        subreddits: Optional[List[str]] = None,
        min_relevance: float = 0.1,
        days_back: int = 7,
    ) -> Dict:
        """
        åˆ†ææŒ‡å®šè‚¡ç¥¨çš„çƒ­åº¦

        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            subreddits: è¦åˆ†æçš„subredditåˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨STOCK_SUBREDDITS
            min_relevance: æœ€å°ç›¸å…³åº¦é˜ˆå€¼
            days_back: åˆ†æè¿‡å»å‡ å¤©çš„æ•°æ®

        Returns:
            Dict: åˆ†æç»“æœ
        """
        if subreddits is None:
            subreddits = STOCK_SUBREDDITS

        keywords = self.generate_stock_keywords(ticker)

        # è®¡ç®—æ—¶é—´èŒƒå›´
        cutoff_time = time.time() - (days_back * 24 * 3600)

        results = {
            "ticker": ticker,
            "keywords": keywords,
            "analysis_period_days": days_back,
            "total_mentions": 0,
            "total_popularity_score": 0.0,
            "subreddit_breakdown": {},
            "top_posts": [],
            "average_sentiment": 0.0,
            "timestamp": datetime.now().isoformat(),
        }

        all_relevant_posts = []

        for subreddit_name in subreddits:
            subreddit_data = {
                "name": subreddit_name,
                "mentions": 0,
                "popularity_score": 0.0,
                "posts": [],
            }

            # è·å–subredditæƒé‡
            weight = SUBREDDIT_WEIGHTS.get(subreddit_name, 0.5)

            # è¯»å–subredditæ•°æ®æ–‡ä»¶
            data_file = self.data_dir / "company_news" / f"{subreddit_name}.jsonl"
            if not data_file.exists():
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
                continue

            try:
                with open(data_file, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue

                        try:
                            post = json.loads(line)

                            # æ—¶é—´è¿‡æ»¤
                            if post.get("created_utc", 0) < cutoff_time:
                                continue

                            # è®¡ç®—ç›¸å…³åº¦
                            relevance = self.calculate_post_relevance(post, keywords)
                            if relevance < min_relevance:
                                continue

                            # è®¡ç®—çƒ­åº¦åˆ†æ•°
                            popularity_score = self.calculate_post_popularity_score(
                                post, weight
                            )

                            # æ·»åŠ åˆ°ç»“æœ
                            post_result = {
                                "id": post.get("id"),
                                "title": post.get("title"),
                                "subreddit": subreddit_name,
                                "score": post.get("score", 0),
                                "comments": post.get("num_comments", 0),
                                "upvotes": post.get("ups", 0),
                                "relevance": relevance,
                                "popularity_score": popularity_score,
                                "url": post.get("permalink", ""),
                                "created_utc": post.get("created_utc"),
                            }

                            subreddit_data["posts"].append(post_result)
                            all_relevant_posts.append(post_result)
                            subreddit_data["mentions"] += 1
                            subreddit_data["popularity_score"] += popularity_score

                        except json.JSONDecodeError as e:
                            logger.warning(f"âš ï¸ JSONè§£æé”™è¯¯: {e}")
                            continue

            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {data_file}: {e}")
                continue

            results["subreddit_breakdown"][subreddit_name] = subreddit_data

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        results["total_mentions"] = len(all_relevant_posts)
        results["total_popularity_score"] = sum(
            post["popularity_score"] for post in all_relevant_posts
        )

        # è·å–çƒ­é—¨å¸–å­ (æŒ‰çƒ­åº¦åˆ†æ•°æ’åº)
        all_relevant_posts.sort(key=lambda x: x["popularity_score"], reverse=True)
        results["top_posts"] = all_relevant_posts[:10]  # å–å‰10ä¸ª

        # è®¡ç®—å¹³å‡åˆ†æ•°
        if results["total_mentions"] > 0:
            results["average_popularity_score"] = (
                results["total_popularity_score"] / results["total_mentions"]
            )
        else:
            results["average_popularity_score"] = 0.0

        logger.info(
            f"ğŸ“Š {ticker} åˆ†æå®Œæˆ: {results['total_mentions']} æ¬¡æåŠ, æ€»çƒ­åº¦: {results['total_popularity_score']:.2f}"
        )

        return results

    def generate_stock_popularity_ranking(
        self,
        tickers: Optional[List[str]] = None,
        subreddits: Optional[List[str]] = None,
        min_relevance: float = 0.1,
        days_back: int = 7,
        top_n: int = 20,
    ) -> Dict:
        """
        ç”Ÿæˆè‚¡ç¥¨çƒ­åº¦æ’è¡Œæ¦œ

        Args:
            tickers: è¦åˆ†æçš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨ticker_to_companyä¸­çš„æ‰€æœ‰è‚¡ç¥¨
            subreddits: è¦åˆ†æçš„subredditåˆ—è¡¨
            min_relevance: æœ€å°ç›¸å…³åº¦é˜ˆå€¼
            days_back: åˆ†æè¿‡å»å‡ å¤©çš„æ•°æ®
            top_n: è¿”å›å‰Nå

        Returns:
            Dict: æ’è¡Œæ¦œç»“æœ
        """
        if tickers is None:
            tickers = list(ticker_to_company.keys())

        logger.info(f"ğŸ† å¼€å§‹ç”Ÿæˆè‚¡ç¥¨çƒ­åº¦æ’è¡Œæ¦œ (åˆ†æ {len(tickers)} åªè‚¡ç¥¨)")

        rankings = []

        with tqdm(tickers, desc="åˆ†æè‚¡ç¥¨çƒ­åº¦") as pbar:
            for ticker in pbar:
                pbar.set_description(f"åˆ†æ {ticker}")

                try:
                    analysis = self.analyze_stock_popularity(
                        ticker=ticker,
                        subreddits=subreddits,
                        min_relevance=min_relevance,
                        days_back=days_back,
                    )

                    # åªåŒ…å«æœ‰æåŠçš„è‚¡ç¥¨
                    if analysis["total_mentions"] > 0:
                        ranking_entry = {
                            "rank": 0,  # ç¨åå¡«å…¥
                            "ticker": ticker,
                            "company_name": ticker_to_company.get(ticker, ticker),
                            "total_mentions": analysis["total_mentions"],
                            "total_popularity_score": analysis[
                                "total_popularity_score"
                            ],
                            "average_popularity_score": analysis[
                                "average_popularity_score"
                            ],
                            "top_subreddit": self._get_top_subreddit(
                                analysis["subreddit_breakdown"]
                            ),
                            "trend_description": self._generate_trend_description(
                                analysis
                            ),
                            "sample_posts": analysis["top_posts"][
                                :3
                            ],  # åªå–å‰3ä¸ªä»£è¡¨æ€§å¸–å­
                        }
                        rankings.append(ranking_entry)

                except Exception as e:
                    logger.error(f"âŒ åˆ†æ {ticker} å¤±è´¥: {e}")
                    continue

                # é¿å…APIé™åˆ¶
                time.sleep(0.1)

        # æŒ‰æ€»çƒ­åº¦åˆ†æ•°æ’åº
        rankings.sort(key=lambda x: x["total_popularity_score"], reverse=True)

        # æ·»åŠ æ’å
        for i, entry in enumerate(rankings[:top_n]):
            entry["rank"] = i + 1

        # ç”Ÿæˆæ’è¡Œæ¦œç»“æœ
        result = {
            "generated_at": datetime.now().isoformat(),
            "analysis_period_days": days_back,
            "total_stocks_analyzed": len(tickers),
            "stocks_with_mentions": len(rankings),
            "top_stocks": rankings[:top_n],
            "summary_stats": self._generate_summary_stats(rankings[:top_n]),
        }

        logger.info(f"ğŸ‰ æ’è¡Œæ¦œç”Ÿæˆå®Œæˆ! å…±å‘ç° {len(rankings)} åªæœ‰è®¨è®ºçš„è‚¡ç¥¨")

        return result

    def _get_top_subreddit(self, subreddit_breakdown: Dict) -> str:
        """è·å–è®¨è®ºæœ€å¤šçš„subreddit"""
        if not subreddit_breakdown:
            return ""

        top_subreddit = max(subreddit_breakdown.items(), key=lambda x: x[1]["mentions"])
        return top_subreddit[0]

    def _generate_trend_description(self, analysis: Dict) -> str:
        """ç”Ÿæˆè¶‹åŠ¿æè¿°"""
        mentions = analysis["total_mentions"]

        if mentions == 0:
            return "æ— è®¨è®º"
        elif mentions < 5:
            return "è½»åº¦è®¨è®º"
        elif mentions < 20:
            return "ä¸­ç­‰è®¨è®º"
        elif mentions < 50:
            return "æ´»è·ƒè®¨è®º"
        else:
            return "çƒ­é—¨è®¨è®º"

    def _generate_summary_stats(self, top_stocks: List[Dict]) -> Dict:
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        if not top_stocks:
            return {}

        total_mentions = sum(stock["total_mentions"] for stock in top_stocks)
        total_score = sum(stock["total_popularity_score"] for stock in top_stocks)

        return {
            "total_mentions_all_stocks": total_mentions,
            "total_popularity_score_all_stocks": total_score,
            "average_mentions_per_stock": total_mentions / len(top_stocks),
            "most_discussed_stock": top_stocks[0]["ticker"] if top_stocks else "",
            "hottest_stock": max(
                top_stocks, key=lambda x: x["average_popularity_score"]
            )["ticker"]
            if top_stocks
            else "",
        }

    def print_popularity_ranking(
        self, ranking_result: Dict, show_details: bool = False
    ):
        """
        ç¾è§‚åœ°æ‰“å°è‚¡ç¥¨çƒ­åº¦æ’è¡Œæ¦œ

        Args:
            ranking_result: generate_stock_popularity_rankingçš„è¿”å›ç»“æœ
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        output = self.format_popularity_ranking(ranking_result, show_details)
        print(output)

    def format_popularity_ranking(
        self,
        ranking_result: Dict,
        show_details: bool = False,
        include_full_posts: bool = False,
    ) -> str:
        """
        æ ¼å¼åŒ–è‚¡ç¥¨çƒ­åº¦æ’è¡Œæ¦œä¸ºå­—ç¬¦ä¸²

        Args:
            ranking_result: generate_stock_popularity_rankingçš„è¿”å›ç»“æœ
            show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            include_full_posts: æ˜¯å¦åŒ…å«å®Œæ•´çš„å¸–å­å†…å®¹

        Returns:
            str: æ ¼å¼åŒ–åçš„æ’è¡Œæ¦œå­—ç¬¦ä¸²
        """
        lines = []

        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        lines.append("ğŸ† Redditè‚¡ç¥¨çƒ­åº¦æ’è¡Œæ¦œ")
        lines.append("=" * 60)
        lines.append(f"ğŸ“… åˆ†ææ—¶é—´æ®µ: æœ€è¿‘ {ranking_result['analysis_period_days']} å¤©")
        lines.append(f"ğŸ“Š åˆ†æè‚¡ç¥¨æ€»æ•°: {ranking_result['total_stocks_analyzed']}")
        lines.append(f"ğŸ’¬ æœ‰è®¨è®ºçš„è‚¡ç¥¨: {ranking_result['stocks_with_mentions']}")
        lines.append(f"â° ç”Ÿæˆæ—¶é—´: {ranking_result['generated_at']}")
        lines.append("")

        top_stocks = ranking_result["top_stocks"]

        # æ’è¡Œæ¦œ
        lines.append("ğŸ“ˆ çƒ­åº¦æ’è¡Œæ¦œ:")
        lines.append("-" * 60)
        lines.append(
            f"{'æ’å':<4} {'è‚¡ç¥¨':<6} {'å…¬å¸å':<20} {'æåŠ':<6} {'çƒ­åº¦':<8} {'è¶‹åŠ¿':<8}"
        )
        lines.append("-" * 60)

        for stock in top_stocks:
            lines.append(
                f"{stock['rank']:<4} {stock['ticker']:<6} {stock['company_name'][:18]:<20} "
                f"{stock['total_mentions']:<6} {stock['total_popularity_score']:<8.1f} {stock['trend_description']:<8}"
            )

        # è¯¦ç»†ä¿¡æ¯
        if show_details and top_stocks:
            lines.append("\nğŸ”¥ çƒ­é—¨è‚¡ç¥¨è¯¦æƒ…:")
            lines.append("-" * 60)

            for i, stock in enumerate(top_stocks[:5]):  # åªæ˜¾ç¤ºå‰5åè¯¦æƒ…
                lines.append(
                    f"\n{stock['rank']}. {stock['ticker']} - {stock['company_name']}"
                )
                lines.append(f"   ğŸ“Š æåŠæ¬¡æ•°: {stock['total_mentions']}")
                lines.append(f"   ğŸ”¥ æ€»çƒ­åº¦: {stock['total_popularity_score']:.1f}")
                lines.append(f"   ğŸ“ ä¸»è¦è®¨è®ºåŒº: r/{stock['top_subreddit']}")

                # å¦‚æœéœ€è¦åŒ…å«å®Œæ•´å¸–å­å†…å®¹ï¼Œåˆ™æ·»åŠ subredditåˆ†å¸ƒè¯¦æƒ…
                if include_full_posts and i == 0:  # åªä¸ºç¬¬ä¸€åæ˜¾ç¤ºè¯¦ç»†åˆ†å¸ƒ
                    # è®¡ç®—subredditåˆ†å¸ƒ
                    subreddit_dist = {}
                    for post in stock["sample_posts"]:
                        subreddit = post.get("subreddit", "unknown")
                        if subreddit not in subreddit_dist:
                            subreddit_dist[subreddit] = {"count": 0, "total_score": 0}
                        subreddit_dist[subreddit]["count"] += 1
                        subreddit_dist[subreddit]["total_score"] += post.get(
                            "popularity_score", 0
                        )

                    if subreddit_dist:
                        lines.append("   ğŸ“‹ å„è®¨è®ºåŒºåˆ†å¸ƒ:")
                        sorted_subreddits = sorted(
                            subreddit_dist.items(),
                            key=lambda x: x[1]["count"],
                            reverse=True,
                        )
                        for subreddit, data in sorted_subreddits:
                            lines.append(
                                f"      r/{subreddit}: {data['count']}æ¬¡æåŠ, æ€»çƒ­åº¦{data['total_score']:.1f}"
                            )
                        lines.append("")

                if stock["sample_posts"]:
                    lines.append("   ğŸ“ çƒ­é—¨å¸–å­:")
                    post_limit = (
                        len(stock["sample_posts"])
                        if include_full_posts
                        else min(3, len(stock["sample_posts"]))
                    )

                    for j, post in enumerate(stock["sample_posts"][:post_limit]):
                        if include_full_posts:
                            # å®Œæ•´å¸–å­ä¿¡æ¯
                            lines.append(f"      {j + 1}. æ ‡é¢˜: {post['title']}")
                            lines.append(f"         æ¥æº: r/{post['subreddit']}")
                            lines.append(
                                f"         äº’åŠ¨: ğŸ‘{post['upvotes']} ğŸ’¬{post['comments']} åˆ†æ•°:{post['score']}"
                            )
                            lines.append(f"         ç›¸å…³åº¦: {post['relevance']:.2f}")
                            lines.append(
                                f"         çƒ­åº¦åˆ†æ•°: {post['popularity_score']:.1f}"
                            )
                            if post.get("url"):
                                lines.append(f"         é“¾æ¥: {post['url']}")
                                # pass
                            lines.append("")
                        else:
                            # ç®€åŒ–ä¿¡æ¯
                            lines.append(f"      {j + 1}. {post['title'][:80]}...")
                            lines.append(
                                f"         (ğŸ‘{post['upvotes']} ğŸ’¬{post['comments']} "
                                f"ç›¸å…³åº¦:{post['relevance']:.2f} çƒ­åº¦:{post['popularity_score']:.1f})"
                            )

        # æ±‡æ€»ç»Ÿè®¡
        if "summary_stats" in ranking_result:
            stats = ranking_result["summary_stats"]
            lines.append("\nğŸ“ˆ æ±‡æ€»ç»Ÿè®¡:")
            lines.append(f"   ğŸ¯ æœ€å—è®¨è®º: {stats.get('most_discussed_stock', 'N/A')}")
            lines.append(f"   ğŸ”¥ æœ€çƒ­é—¨: {stats.get('hottest_stock', 'N/A')}")
            lines.append(
                f"   ğŸ’¬ å¹³å‡æåŠ: {stats.get('average_mentions_per_stock', 0):.1f} æ¬¡/è‚¡ç¥¨"
            )

        return "\n".join(lines)


# ä¾¿æ·APIå‡½æ•°
def analyze_stock_popularity(
    ticker: str,
    days_back: int = 7,
    min_relevance: float = 0.1,
    data_dir: Optional[str] = None,
) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ†æå•åªè‚¡ç¥¨çš„Redditçƒ­åº¦

    Args:
        ticker: è‚¡ç¥¨ä»£ç  (å¦‚ "AAPL")
        days_back: åˆ†æè¿‡å»å‡ å¤©çš„æ•°æ®
        min_relevance: æœ€å°ç›¸å…³åº¦é˜ˆå€¼
        data_dir: æ•°æ®ç›®å½•

    Returns:
        Dict: åˆ†æç»“æœ
    """
    analyzer = StockPopularityAnalyzer(data_dir=data_dir)
    return analyzer.analyze_stock_popularity(
        ticker=ticker, min_relevance=min_relevance, days_back=days_back
    )


def generate_reddit_stock_ranking(
    top_n: int = 20,
    days_back: int = 7,
    tickers: Optional[List[str]] = None,
    data_dir: Optional[str] = None,
    print_results: bool = True,
    show_details: bool = False,
) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šç”ŸæˆRedditè‚¡ç¥¨çƒ­åº¦æ’è¡Œæ¦œ

    Args:
        top_n: è¿”å›å‰Nå
        days_back: åˆ†æè¿‡å»å‡ å¤©çš„æ•°æ®
        tickers: è¦åˆ†æçš„è‚¡ç¥¨åˆ—è¡¨ï¼Œé»˜è®¤åˆ†ææ‰€æœ‰å·²é…ç½®çš„è‚¡ç¥¨
        data_dir: æ•°æ®ç›®å½•
        print_results: æ˜¯å¦æ‰“å°ç»“æœ
        show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        Dict: æ’è¡Œæ¦œç»“æœ
    """
    analyzer = StockPopularityAnalyzer(data_dir=data_dir)

    ranking_result = analyzer.generate_stock_popularity_ranking(
        tickers=tickers, days_back=days_back, top_n=top_n
    )

    # if print_results:
    #     analyzer.print_popularity_ranking(ranking_result, show_details=show_details)

    formatted_text = analyzer.format_popularity_ranking(
        ranking_result, show_details=True, include_full_posts=True
    )

    return formatted_text


def format_reddit_stock_ranking(
    top_n: int = 20,
    days_back: int = 7,
    tickers: Optional[List[str]] = None,
    data_dir: Optional[str] = None,
    show_details: bool = True,
    include_full_posts: bool = True,
) -> str:
    """
    ä¾¿æ·å‡½æ•°ï¼šç”ŸæˆRedditè‚¡ç¥¨çƒ­åº¦æ’è¡Œæ¦œå¹¶è¿”å›æ ¼å¼åŒ–å­—ç¬¦ä¸²

    Args:
        top_n: è¿”å›å‰Nå
        days_back: åˆ†æè¿‡å»å‡ å¤©çš„æ•°æ®
        tickers: è¦åˆ†æçš„è‚¡ç¥¨åˆ—è¡¨ï¼Œé»˜è®¤åˆ†ææ‰€æœ‰å·²é…ç½®çš„è‚¡ç¥¨
        data_dir: æ•°æ®ç›®å½•
        show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        include_full_posts: æ˜¯å¦åŒ…å«å®Œæ•´çš„å¸–å­å†…å®¹

    Returns:
        str: æ ¼å¼åŒ–çš„æ’è¡Œæ¦œå­—ç¬¦ä¸²
    """
    analyzer = StockPopularityAnalyzer(data_dir=data_dir)

    ranking_result = analyzer.generate_stock_popularity_ranking(
        tickers=tickers, days_back=days_back, top_n=top_n
    )

    return analyzer.format_popularity_ranking(
        ranking_result, show_details=show_details, include_full_posts=include_full_posts
    )


def get_trending_stocks(
    days_back: int = 1, min_mentions: int = 5, data_dir: Optional[str] = None
) -> List[Dict]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–è¿‘æœŸçƒ­é—¨è‚¡ç¥¨

    Args:
        days_back: åˆ†æè¿‡å»å‡ å¤©çš„æ•°æ®
        min_mentions: æœ€å°‘æåŠæ¬¡æ•°é˜ˆå€¼
        data_dir: æ•°æ®ç›®å½•

    Returns:
        List[Dict]: çƒ­é—¨è‚¡ç¥¨åˆ—è¡¨
    """
    analyzer = StockPopularityAnalyzer(data_dir=data_dir)

    ranking_result = analyzer.generate_stock_popularity_ranking(
        days_back=days_back,
        top_n=50,  # è·å–æ›´å¤šå€™é€‰
    )

    # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„çƒ­é—¨è‚¡ç¥¨
    trending_stocks = [
        stock
        for stock in ranking_result["top_stocks"]
        if stock["total_mentions"] >= min_mentions
    ]

    return trending_stocks


def compare_stock_popularity(
    tickers: List[str], days_back: int = 7, data_dir: Optional[str] = None
) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ¯”è¾ƒå¤šåªè‚¡ç¥¨çš„çƒ­åº¦

    Args:
        tickers: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        days_back: åˆ†æè¿‡å»å‡ å¤©çš„æ•°æ®
        data_dir: æ•°æ®ç›®å½•

    Returns:
        Dict: æ¯”è¾ƒç»“æœ
    """
    analyzer = StockPopularityAnalyzer(data_dir=data_dir)

    comparisons = {}

    for ticker in tickers:
        try:
            analysis = analyzer.analyze_stock_popularity(
                ticker=ticker, days_back=days_back
            )
            comparisons[ticker] = {
                "mentions": analysis["total_mentions"],
                "popularity_score": analysis["total_popularity_score"],
                "average_score": analysis["average_popularity_score"],
                "company_name": ticker_to_company.get(ticker, ticker),
            }
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ†æ {ticker} å¤±è´¥: {e}")
            comparisons[ticker] = None

    # æŒ‰çƒ­åº¦æ’åº
    valid_comparisons = {k: v for k, v in comparisons.items() if v is not None}
    sorted_tickers = sorted(
        valid_comparisons.items(), key=lambda x: x[1]["popularity_score"], reverse=True
    )

    result = {
        "comparison_date": datetime.now().isoformat(),
        "analysis_period_days": days_back,
        "tickers_analyzed": tickers,
        "successful_analyses": len(valid_comparisons),
        "rankings": sorted_tickers,
        "winner": sorted_tickers[0][0] if sorted_tickers else None,
    }

    return result


def download_and_analyze_stocks(
    tickers: List[str],
    subreddit_category: str = "company_news",
    limit_per_subreddit: int = 100,
    analysis_days: int = 7,
    data_dir: Optional[str] = None,
    need_download: bool = False,
) -> Dict:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸€é”®ä¸‹è½½æ•°æ®å¹¶åˆ†æè‚¡ç¥¨çƒ­åº¦

    Args:
        tickers: è¦åˆ†æçš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
        subreddit_category: subredditåˆ†ç±»
        limit_per_subreddit: æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶
        analysis_days: åˆ†æå¤©æ•°
        data_dir: æ•°æ®ç›®å½•

    Returns:
        Dict: åˆ†æç»“æœ
    """
    logger.info("ğŸš€ å¼€å§‹ä¸€é”®ä¸‹è½½å¹¶åˆ†æè‚¡ç¥¨çƒ­åº¦")

    # æ­¥éª¤1: ä¸‹è½½æœ€æ–°æ•°æ®
    logger.info("ğŸ“¥ æ­¥éª¤1: ä¸‹è½½Redditæ•°æ®...")
    if need_download:
        download_reddit_data(
            category=subreddit_category,
            limit_per_subreddit=limit_per_subreddit,
            data_dir=data_dir,
            force_refresh=True,
        )

    # æ­¥éª¤2: åˆ†æè‚¡ç¥¨çƒ­åº¦
    logger.info("ğŸ“Š æ­¥éª¤2: åˆ†æè‚¡ç¥¨çƒ­åº¦...")
    analysis_result = generate_reddit_stock_ranking(
        top_n=len(tickers),
        days_back=analysis_days,
        tickers=tickers,
        data_dir=data_dir,
        print_results=True,
        show_details=True,
    )

    return {
        "analysis_result": analysis_result,
        "summary": f"æˆåŠŸåˆ†æ {len(tickers)} åªè‚¡ç¥¨çš„Redditçƒ­åº¦",
    }
