#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Redditæ•°æ®ä¸‹è½½è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºä»Reddit APIä¸‹è½½æ–°é—»å’Œè®¨è®ºæ•°æ®ã€‚
æ”¯æŒæ‰¹é‡ä¸‹è½½å¤šä¸ªsubredditçš„æ•°æ®ï¼Œå¹¶ä¿å­˜ä¸ºJSONLæ ¼å¼ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/download_reddit_data.py --category company_news --limit 100
    python scripts/download_reddit_data.py --category all --limit 50
    python scripts/download_reddit_data.py --demo
"""

import os
import sys
import argparse
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from tradingagents.dataflows.reddit_utils import (
        download_reddit_data,
        download_custom_subreddits,
        demo_usage,
        PRAW_AVAILABLE,
    )
    from tradingagents.utils.logging_manager import get_logger

    logger = get_logger("reddit_downloader")
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


def check_requirements():
    """æ£€æŸ¥è¿è¡Œè¦æ±‚"""
    print("ğŸ” æ£€æŸ¥è¿è¡Œè¦æ±‚...")

    # æ£€æŸ¥prawåº“
    if not PRAW_AVAILABLE:
        print("âŒ prawåº“æœªå®‰è£…")
        print("ğŸ’¡ è¯·è¿è¡Œ: pip install praw")
        return False

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")

    if not all([client_id, client_secret]):
        print("âŒ Reddit APIå‡­è¯æœªé…ç½®")
        print("ğŸ’¡ è¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
        print("   REDDIT_CLIENT_ID=your_client_id")
        print("   REDDIT_CLIENT_SECRET=your_client_secret")
        print("   REDDIT_USER_AGENT=YourApp/1.0 (å¯é€‰)")
        return False

    print("âœ… æ‰€æœ‰è¦æ±‚å·²æ»¡è¶³")
    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Redditæ•°æ®ä¸‹è½½å·¥å…·")
    parser.add_argument(
        "--category",
        default="company_news",
        choices=["all", "global_news", "company_news", "crypto_news"],
        help="è¦ä¸‹è½½çš„åˆ†ç±»",
    )
    parser.add_argument(
        "--limit", type=int, default=100, help="æ¯ä¸ªsubredditçš„ä¸‹è½½é™åˆ¶"
    )
    parser.add_argument(
        "--type",
        default="hot",
        choices=["hot", "new", "top", "rising"],
        help="å¸–å­åˆ†ç±»",
    )
    parser.add_argument(
        "--time-filter",
        default="week",
        choices=["all", "day", "week", "month", "year"],
        help="æ—¶é—´ç­›é€‰ (ä»…å¯¹topç±»å‹æœ‰æ•ˆ)",
    )
    parser.add_argument(
        "--force-refresh", action="store_true", help="å¼ºåˆ¶åˆ·æ–°å·²å­˜åœ¨çš„æ–‡ä»¶"
    )
    parser.add_argument("--demo", action="store_true", help="æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹")
    parser.add_argument(
        "--data-dir", default=None, help="æ•°æ®å­˜å‚¨ç›®å½• (é»˜è®¤: data/reddit_data)"
    )
    parser.add_argument("--custom-subreddits", nargs="+", help="è‡ªå®šä¹‰subredditåˆ—è¡¨")
    parser.add_argument("--check", action="store_true", help="æ£€æŸ¥è¿è¡Œè¦æ±‚")

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    # æ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
    if args.demo:
        demo_usage()
        return

    # æ£€æŸ¥è¦æ±‚
    if args.check:
        if check_requirements():
            print("ğŸ‰ ç¯å¢ƒé…ç½®æ­£ç¡®ï¼Œå¯ä»¥å¼€å§‹ä¸‹è½½æ•°æ®")
        else:
            print("âŒ ç¯å¢ƒé…ç½®æœ‰é—®é¢˜ï¼Œè¯·å…ˆè§£å†³")
        return

    # æ£€æŸ¥è¿è¡Œè¦æ±‚
    if not check_requirements():
        sys.exit(1)

    print("ğŸš€ å¼€å§‹ä¸‹è½½Redditæ•°æ®")
    print("=" * 50)
    print(f"ğŸ“‚ åˆ†ç±»: {args.category}")
    print(f"ğŸ“Š é™åˆ¶: {args.limit}")
    print(f"ğŸ·ï¸ ç±»å‹: {args.type}")
    print(f"â° æ—¶é—´ç­›é€‰: {args.time_filter}")
    print(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°: {args.force_refresh}")
    if args.data_dir:
        print(f"ğŸ“ æ•°æ®ç›®å½•: {args.data_dir}")
    print("")

    try:
        if args.custom_subreddits:
            # ä¸‹è½½è‡ªå®šä¹‰subreddit
            print(f"ğŸ“‹ è‡ªå®šä¹‰Subreddits: {args.custom_subreddits}")
            result = download_custom_subreddits(
                subreddits=args.custom_subreddits,
                category_name="custom",
                limit_per_subreddit=args.limit,
                category_type=args.type,
                time_filter=args.time_filter,
                force_refresh=args.force_refresh,
                data_dir=args.data_dir,
            )
            results = {"custom": result}
        else:
            # ä¸‹è½½é¢„é…ç½®åˆ†ç±»
            results = download_reddit_data(
                category=args.category,
                limit_per_subreddit=args.limit,
                category_type=args.type,
                time_filter=args.time_filter,
                force_refresh=args.force_refresh,
                data_dir=args.data_dir,
            )

        print("\nğŸ‰ ä¸‹è½½å®Œæˆ!")
        print("ğŸ“Š ç»“æœ:")
        for category, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            print(f"   {category}: {status}")

        # æ˜¾ç¤ºæ•°æ®ä½ç½®
        data_dir = args.data_dir or "data/reddit_data"
        print(f"\nğŸ“ æ•°æ®ä¿å­˜ä½ç½®: {data_dir}")
        print("ğŸ’¡ æ•°æ®æ ¼å¼: æ¯ä¸ªsubredditä¿å­˜ä¸ºå•ç‹¬çš„.jsonlæ–‡ä»¶")

    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
